\documentclass[12pt]{article}
%------------------------------Packages généraux------------------------------

\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{ae}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=2cm,bottom=3cm]{geometry}
%------------------------------Mathématiques------------------------------

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{eucal}
\usepackage{array}


%------------------------------Graphics------------------------------

\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{fancybox}
\usepackage{color}
\usepackage{epstopdf}
\usepackage{float}
\usepackage{diagbox}
\usepackage{svg}

%------------------------------Syntaxe------------------------------

\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
\lstloadlanguages{Matlab}

\def\refmark#1{\hbox{$^{\ref{#1}}$}}
\DeclareSymbolFont{cmmathcal}{OMS}{cmsy}{m}{n} %Mathcal correcte
\DeclareSymbolFontAlphabet{\mathcal}{cmmathcal}

\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}


\begin{document} 
\begin{titlepage}


  \begin{sffamily}
  \begin{center}

   
    \textsc{\Large University of Liège}\\[0.8cm]
    
    \begin{figure}[h!]
		\begin{center}
		\includegraphics[scale=0.4]{logo_ulg.jpg}\\[1cm]
		\end{center}
	\end{figure}
	

    \textsc{\Large Machine Learning}\\[1.1cm]

    
    \HRule \\[0.4cm]
    { \LARGE \bfseries Classification algorithms\\[0.4cm] }

    \HRule \\[0.5cm]
    
    \textsc{Master 1 in Data Science \& Engineering} \\[2.5cm]

   \begin{minipage}{0.4\textwidth}
      \begin{flushleft} \large
        \emph{Authors:}\\
        Tom \textsc{Crasset}\\
        Antoine \textsc{Louis} 
      \end{flushleft}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
      \begin{flushright} \large
        \emph{Professors :}\\
        L. \textsc{Wehenkel}\\
        P. \textsc{Geurts}\\

      \end{flushright}
    \end{minipage}
    


    \vfill

    
    {\large Academic year 2018-2019}

  \end{center}
  \end{sffamily}
\end{titlepage}




\section{Decision tree}

\subsection{Influence of the depth on the decision boundary}

\begin{figure}[htbp]
\centering
\begin{minipage}{0.3\linewidth}%
  \centering
  \includesvg[width=\textwidth]{"DT maxdepth 1WP"}%
\subcaption[first caption.]{}\label{fig:1a}%
\end{minipage}%
\begin{minipage}{0.3\linewidth}%
  \centering
  \includegraphics[width=\textwidth]{"DT maxdepth 2"}%
\subcaption[second caption.]{}\label{fig:1b}%
\end{minipage}%
\begin{minipage}{0.3\linewidth}%
  \centering
  \includegraphics[width=\textwidth]{"DT maxdepth 4"}%
\subcaption[third caption.]{}\label{fig:1c}%
\end{minipage}
\newline
\centering
\begin{minipage}{0.3\linewidth}%
  \centering
  \includegraphics[width=\textwidth]{"DT maxdepth 8"}%
\subcaption[fourth caption.]{}\label{fig:1d}%
\end{minipage}%
\begin{minipage}{0.3\linewidth}%
  \centering
  \includegraphics[width=\textwidth]{"DT maxdepth None"}%
\subcaption[fifth caption.]{}\label{fig:1e}%
\end{minipage}%

\caption{ \label{fig:maxdepth} Decision boundary for a \texttt{max\_depth} value of (\subref{fig:1a}) 1, (\subref{fig:1b}) 2, (\subref{fig:1c}) 4, (\subref{fig:1d}) 8 and (\subref{fig:1e}) None. The points are intentionally left out of the first 4 figures to the decision boundary}
\end{figure}

As can be seen on Figure \ref{fig:maxdepth}, a \texttt{max\_depth} value of 1 splits the state space in two, placing the decision boundary horizontally in the middle of the set. The model doesn't properly fit the data at all and is clearly underfitting. The ellipsoid shape of the blue data points isn't detected at all, neither is the circular shape of the red ones.

Increasing the depth further to 2 introduces a linear separation, this time vertically, to select a majority of blue points.

As the depth even further increases, more and more linear separations will take place and thus will create rectangular surfaces all around the data points, sometimes trying to fit lonely data points inside another dense pocket of other coloured points, such as in Figure \ref{fig:1d} or Figure \ref{fig:1e} with a maxdepth of 8 or no constraint on the maximal depth. These clearly overfit the data, sometimes creating surfaces which don't even include a single data points they are trying to classify.

A maximal depth of 4 seems to be the ideal compromise for such a decision tree classifier as it captures the majority of the points without trying to contort itself around lonely data points and thereby skewing the accuracy.

The claim that the model seems to be more confident with an unconstrained maximal depth contradicts the results that came out of the testing. Indeed, the accuracy worsened as well as the variance on all the generations as can be seen on Table \ref{tab:mean_dt} in the following point.

\subsection{Average test set accuracies}

The Table \ref{tab:mean_dt} reports the average test set accuracies over five generations of the dataset 2 along with the standard deviation for each depth. It confirms what was previously said : with a maxdepth of 1, the model is clearly underfitting whereas it is clearly overfitting with no constraint on the depth. The maximal depth of 4 seems to be the ideal one with the best average accuracy and the lowest standard deviation over the five data generations.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
  \hline
 & 1 & 2 & 4 & 8 & None \\
  \hline
  Average & 0.667 & 0.819 & 0.833 & 0.832 & 0.791 \\
  \hline
  Standard deviation & 0.0491 & 0.0174 & 0.0098 & 0.0192 & 0.0304 \\
  \hline
\end{tabular}
    \caption{Average test set accuracies and standard deviation for each depth}
    \label{tab:mean_dt}
\end{table}


\section{K-nearest neighbours}

\subsection{Influence of the n\_neighbours on the decision boundary}

\begin{figure}[htbp]
\centering
\begin{minipage}{0.3\linewidth}%
  \centering
  \includegraphics[width=\textwidth]{"KNN neighbors 1"}%
\subcaption[first caption.]{}\label{fig:KNN1a}%
\end{minipage}%
\begin{minipage}{0.3\linewidth}%
  \centering
  \includegraphics[width=\textwidth]{"KNN neighbors 5"}%
\subcaption[second caption.]{}\label{fig:KNN1b}%
\end{minipage}%
\begin{minipage}{0.3\linewidth}%
  \centering
  \includegraphics[width=\textwidth]{"KNN neighbors 25"}%
\subcaption[third caption.]{}\label{fig:KNN1c}%
\end{minipage}
\newline
\centering
\begin{minipage}{0.3\linewidth}%
  \centering
  \includegraphics[width=\textwidth]{"KNN neighbors 125"}%
\subcaption[fourth caption.]{}\label{fig:KNN1d}%
\end{minipage}%https://www.overleaf.com/1549482925dbhvdpbwffkq
\begin{minipage}{0.3\linewidth}%
  \centering
  \includegraphics[width=\textwidth]{"KNN neighbors 625"}%
\subcaption[fifth caption.]{}\label{fig:KNN1e}%
\end{minipage}%
\begin{minipage}{0.3\linewidth}%
  \centering
  \includesvg[width=\textwidth]{"KNN neighbors 1200WP"}%
\subcaption[sixth caption.]{}\label{fig:KNN1f}%
\end{minipage}%

\caption{ \label{fig:nneighbors} Decision boundary for a \texttt{n\_neighbours} value of (\subref{fig:KNN1a}) 1, (\subref{fig:KNN1b}) 5, (\subref{fig:KNN1c}) 25, (\subref{fig:KNN1d}) 125, (\subref{fig:KNN1e}) 625 and (\subref{fig:KNN1f}) 1200.
The points are intentionally left out of the first 5 figures to show more detail of the decision boundary}
\end{figure}


The Figure \ref{fig:nneighbors} shows the different decision boundaries for varying values of \texttt{n\_neighbours}. As one can see, with a value of 1 neighbour, the model clearly underfits the data, coloring regions on the upper portion of the graph in red, even though red points are nowhere near that region. As the number of neighbours increases, the model grows more confident and is able to better map the regions. Whereas there is still a faint red color on the upper portion of the graph for a value of 5, in the subsequent graphs for higher values, this region is blue again.

However, once a number of neighbours of 625 is used, the regions mapped start to become too broad and begin to fade into each other. This is because the total training set has only 1200  samples and with 625 being more than half of the points, the closest points encompass a large amount of the dataset. It is clearly overfitting.

As a number of 1200 is reached, the whole figure is colored a faint blue and that is understandable. In fact, it is to be expected because all the points are used to classify the points and it just so happens that the blue points won over the red ones, but it could have been reversed. The reason might be the disposition of the blue points, they are more spread apart than the red ones and thus cover a bigger region of the plane.
~
~
~
~
\subsection{Optimisation with a ten-fold cross validation strategy}

Our methodology for the 10-fold cross-validation testing is pretty straight forward. K-fold cross validation consists in separating the dataset into K sections, training on a subset of them and testing on the rest of the subset. To reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation results are averaged over the rounds to give an estimate of the model’s predictive performance. 

In Python, a simple method for doing this is the \texttt{cross\_val\_score()} method.
This is repeated for varying values of \texttt{n\_neighbours} to find the optimal value, that is the value that has the lowest misclassification error.


As expected, the optimal number of neighbours is 125. This confirms the trend that is shown on Figure \ref{fig:nneighbors}. Indeed, the number 125 is neither too high nor too low to accurately classify the points in the space. 125 is more or less 10\% of the size of the training set used.

Additionally, the misclassification error has been plotted for the different values of \texttt{n\_neighbours} on Figure \ref{fig:MSE} and it shows that 125 has the lowest error value and that for 1200 , the error is nearly 50\%, which is equivalent to flipping a coin.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{MSE}
    \caption{Misclassification error for multiple values of \texttt{n\_neighbours}}
    \label{fig:MSE}
\end{figure}


\section{Linear discriminant analysis}

\subsection{Linearity of the decision boundary of LDA}

In the two classes case, it is quite easy to show that the decision boundary of LDA is linear. Each class density being modelled by multivariate Gaussian, LDA arises in the special case when we assume that the classes have a common covariance matrix $\sum_k = \sum \forall k$. In comparing two classes $k$ and $l$, it is sufficient to look at the log-ratio. Then, we see that
\begin{equation}
\begin{aligned}
    log \frac{Pr(G = k|X = x)}{Pr(G = l|X = x)} &= log \frac{f_k(x)}{f_l(x)} + log \frac{\pi_k}{\pi_l}\\
    &= log \frac{\pi_k}{\pi_l} - 0.5(\mu_k + \mu_l)^T \sum^{-1}(\mu_k - \mu_l) + x^T \sum^{-1}(\mu_k - \mu_l),
\end{aligned}
\end{equation}
is an equation linear in x. The equal covariance matrices cause the normalization factors to cancel, as well as the quadratic part in the exponents. This linear log-odds function implies that the decision boundary between classes $k$ and $l$ - the set where $Pr(G = k|X = x) = Pr(G = l|X = x)$ - is linear in $x$, in p dimensions a hyperplane. This is of course true for any pair of classes, so all the decision boundaries are linear. 

If we divide $R^p$ into regions that are classified as class 1, class 2, etc., these regions will be separated by hyperplanes.


\subsection{Decision boundary}

\begin{figure}[htbp]
\centering
\begin{minipage}{0.5\linewidth}%
  \centering
  \includegraphics[width=\textwidth]{"LDA dataset1"}%
\subcaption[first caption.]{}\label{fig:LDA1}%
\end{minipage}%
\begin{minipage}{0.5\linewidth}%
  \centering
  \includegraphics[width=\textwidth]{"LDA dataset2"}%
\subcaption[second caption.]{}\label{fig:LDA2}%
\end{minipage}%

\caption{ \label{fig:LDA} Decision boundary for the LinearDiscriminantAnalysis classifier on (\subref{fig:LDA1}) dataset 1 and (\subref{fig:LDA2}) dataset 2}
\end{figure}

The Figure \ref{fig:LDA} shows that the decision boundary separates the blunt of the two classes quite well, especially for the first data set. 

\subsection{Average test accuracies}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
  \hline
 & Dataset 1 & Dataset 2 \\
  \hline
  Average &  0.903  & 0.839 \\
  \hline
  Standard deviation & 0.0092 & 0.0069 \\
  \hline
\end{tabular}
    \caption{Average test set accuracies and standard deviation for each depth}
    \label{tab:mean_lda}
\end{table}

The Table \ref{tab:mean_lda} reports the average test set accuracies over five generations of the dataset 1 and 2 along with the standard deviation.
The average accuracy for dataset 1 is higher and the Figure \ref{fig:LDA} shows why: in the first dataset, the data clouds of the two classes have the same orientation and the same shape, whereas in the second one, the red class has a more circular shape.
Another thing to note is that the standard deviation is very small for both datasets.


\subsection{Similarities and divergences between datasets}
As briefly commented in the previous point, the shape of the red points could differ from one dataset to the other. 
The fact that the linear discriminant analysis classifier gives better results for the first dataset is because the hypothesis of homoscedasticity holds true. Indeed, the covariance matrix describes the overall shape of the points in a given class and in the case of the dataset 1, the covariance matrices of both classes are indeed the same, so the hypothesis is satisfied. In dataset 2 however, as the Figure \ref{fig:LDA2} shows, the red class has a more circular shape and thus the covariance matrix between the two classes is not the same, the hypothesis is not true and thus the accuracy of the prediction suffers.

\end{document}
